{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting diffusers\n",
      "  Downloading diffusers-0.16.1-py3-none-any.whl (934 kB)\n",
      "     ------------------------------------ 934.9/934.9 kB 924.6 kB/s eta 0:00:00\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.29.0-py3-none-any.whl (7.1 MB)\n",
      "     ---------------------------------------- 7.1/7.1 MB 335.2 kB/s eta 0:00:00\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-0.19.0-py3-none-any.whl (219 kB)\n",
      "     ------------------------------------ 219.1/219.1 kB 669.5 kB/s eta 0:00:00\n",
      "Requirement already satisfied: Pillow in c:\\users\\pjw\\miniconda3\\envs\\first\\lib\\site-packages (from diffusers) (9.3.0)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\pjw\\miniconda3\\envs\\first\\lib\\site-packages (from diffusers) (6.6.0)\n",
      "Collecting huggingface-hub>=0.13.2\n",
      "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
      "     ------------------------------------ 224.5/224.5 kB 978.3 kB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\users\\pjw\\miniconda3\\envs\\first\\lib\\site-packages (from diffusers) (1.24.1)\n",
      "Requirement already satisfied: requests in c:\\users\\pjw\\miniconda3\\envs\\first\\lib\\site-packages (from diffusers) (2.28.1)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2023.5.5-cp38-cp38-win_amd64.whl (267 kB)\n",
      "     -------------------------------------- 267.9/267.9 kB 1.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\pjw\\miniconda3\\envs\\first\\lib\\site-packages (from diffusers) (3.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pjw\\miniconda3\\envs\\first\\lib\\site-packages (from transformers) (23.1)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp38-cp38-win_amd64.whl (3.5 MB)\n",
      "     ---------------------------------------- 3.5/3.5 MB 899.9 kB/s eta 0:00:00\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\pjw\\miniconda3\\envs\\first\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\pjw\\miniconda3\\envs\\first\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\pjw\\miniconda3\\envs\\first\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\pjw\\miniconda3\\envs\\first\\lib\\site-packages (from accelerate) (2.0.1+cu117)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2023.5.0-py3-none-any.whl (160 kB)\n",
      "     ------------------------------------ 160.1/160.1 kB 801.3 kB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\pjw\\miniconda3\\envs\\first\\lib\\site-packages (from huggingface-hub>=0.13.2->diffusers) (4.5.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\pjw\\miniconda3\\envs\\first\\lib\\site-packages (from torch>=1.6.0->accelerate) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\pjw\\miniconda3\\envs\\first\\lib\\site-packages (from torch>=1.6.0->accelerate) (3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\pjw\\miniconda3\\envs\\first\\lib\\site-packages (from torch>=1.6.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\pjw\\miniconda3\\envs\\first\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\pjw\\miniconda3\\envs\\first\\lib\\site-packages (from importlib-metadata->diffusers) (3.15.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\pjw\\miniconda3\\envs\\first\\lib\\site-packages (from requests->diffusers) (1.26.13)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\pjw\\miniconda3\\envs\\first\\lib\\site-packages (from requests->diffusers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pjw\\miniconda3\\envs\\first\\lib\\site-packages (from requests->diffusers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pjw\\miniconda3\\envs\\first\\lib\\site-packages (from requests->diffusers) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\pjw\\miniconda3\\envs\\first\\lib\\site-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\pjw\\miniconda3\\envs\\first\\lib\\site-packages (from sympy->torch>=1.6.0->accelerate) (1.2.1)\n",
      "Installing collected packages: tokenizers, regex, fsspec, huggingface-hub, transformers, diffusers, accelerate\n",
      "Successfully installed accelerate-0.19.0 diffusers-0.16.1 fsspec-2023.5.0 huggingface-hub-0.14.1 regex-2023.5.5 tokenizers-0.13.3 transformers-4.29.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade diffusers transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pjw\\miniconda3\\envs\\first\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\pjw\\Documents\\ai\\ai-data\\stable_diffusion.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pjw/Documents/ai/ai-data/stable_diffusion.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdiffusers\u001b[39;00m \u001b[39mimport\u001b[39;00m StableDiffusionPipeline\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pjw/Documents/ai/ai-data/stable_diffusion.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m pipe \u001b[39m=\u001b[39m StableDiffusionPipeline\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mrunwayml/stable-diffusion-v1-5\u001b[39m\u001b[39m'\u001b[39m, \\\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pjw/Documents/ai/ai-data/stable_diffusion.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                                                torch_dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat16)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/pjw/Documents/ai/ai-data/stable_diffusion.ipynb#W1sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m pipe \u001b[39m=\u001b[39m pipe\u001b[39m.\u001b[39;49mto(\u001b[39m'\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\pjw\\miniconda3\\envs\\first\\lib\\site-packages\\diffusers\\pipelines\\pipeline_utils.py:670\u001b[0m, in \u001b[0;36mDiffusionPipeline.to\u001b[1;34m(self, torch_device, torch_dtype, silence_dtype_warnings)\u001b[0m\n\u001b[0;32m    666\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m    667\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe module \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mmodule\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m has been loaded in 8bit and moving it to \u001b[39m\u001b[39m{\u001b[39;00mtorch_dtype\u001b[39m}\u001b[39;00m\u001b[39m via `.to()` is not yet supported. Module is still on \u001b[39m\u001b[39m{\u001b[39;00mmodule\u001b[39m.\u001b[39mdevice\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    668\u001b[0m     )\n\u001b[0;32m    669\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 670\u001b[0m     module\u001b[39m.\u001b[39;49mto(torch_device, torch_dtype)\n\u001b[0;32m    672\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    673\u001b[0m     module\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39mfloat16\n\u001b[0;32m    674\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mstr\u001b[39m(torch_device) \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    675\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m silence_dtype_warnings\n\u001b[0;32m    676\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_offloaded\n\u001b[0;32m    677\u001b[0m ):\n\u001b[0;32m    678\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m    679\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPipelines loaded with `torch_dtype=torch.float16` cannot run with `cpu` device. It\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    680\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m is not recommended to move them to `cpu` as running them will fail. Please make\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    683\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m `torch_dtype=torch.float16` argument, or use another device for inference.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    684\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\pjw\\miniconda3\\envs\\first\\lib\\site-packages\\transformers\\modeling_utils.py:1886\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1881\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1882\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`.to` is not supported for `8-bit` models. Please use the model as it is, since the\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1883\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m model has already been set to the correct devices and casted to the correct `dtype`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1884\u001b[0m     )\n\u001b[0;32m   1885\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1886\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mto(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\pjw\\miniconda3\\envs\\first\\lib\\site-packages\\torch\\nn\\modules\\module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1141\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m   1143\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m-> 1145\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[1;32mc:\\Users\\pjw\\miniconda3\\envs\\first\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pjw\\miniconda3\\envs\\first\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[1;31m[... skipping similar frames: Module._apply at line 797 (1 times)]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\pjw\\miniconda3\\envs\\first\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pjw\\miniconda3\\envs\\first\\lib\\site-packages\\torch\\nn\\modules\\module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    816\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    817\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    818\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    819\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 820\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[0;32m    821\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    822\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32mc:\\Users\\pjw\\miniconda3\\envs\\first\\lib\\site-packages\\torch\\nn\\modules\\module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1140\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[0;32m   1141\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m-> 1143\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "File \u001b[1;32mc:\\Users\\pjw\\miniconda3\\envs\\first\\lib\\site-packages\\torch\\cuda\\__init__.py:247\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mCUDA_MODULE_LOADING\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39menviron:\n\u001b[0;32m    246\u001b[0m     os\u001b[39m.\u001b[39menviron[\u001b[39m'\u001b[39m\u001b[39mCUDA_MODULE_LOADING\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mLAZY\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 247\u001b[0m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_cuda_init()\n\u001b[0;32m    248\u001b[0m \u001b[39m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[39m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[39m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[0;32m    251\u001b[0m _tls\u001b[39m.\u001b[39mis_initializing \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained('runwayml/stable-diffusion-v1-5', \\\n",
    "                                               torch_dtype=torch.float16)\n",
    "pipe = pipe.to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\pjw\\Documents\\ai\\ai-data\\stable_diffusion.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pjw/Documents/ai/ai-data/stable_diffusion.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39ma photo fo an astronaut riding a horse on mars\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/pjw/Documents/ai/ai-data/stable_diffusion.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m image \u001b[39m=\u001b[39m pipe(prompt)\u001b[39m.\u001b[39mimages[\u001b[39m0\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pjw/Documents/ai/ai-data/stable_diffusion.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m image\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pipe' is not defined"
     ]
    }
   ],
   "source": [
    "prompt = 'a photo fo an astronaut riding a horse on mars'\n",
    "\n",
    "image = pipe(prompt).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "first",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
